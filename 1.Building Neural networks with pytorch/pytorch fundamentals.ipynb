{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5377efa4",
   "metadata": {},
   "source": [
    "## Pytorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34aa6370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca977c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "python_list = [1, 2, 3, 4, 5]\n",
    "tensor = torch.tensor(python_list)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe041a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "python_list_2d = [[1, 2, 3], [4, 5, 6]]\n",
    "tensor_2d = torch.tensor(python_list_2d)\n",
    "print(tensor_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1969c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.int64\n",
      "cpu\n",
      "torch.Size([5])\n",
      "---\n",
      "torch.Size([2, 3])\n",
      "torch.int64\n",
      "cpu\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# print(dir(tensor))\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.device)\n",
    "print(tensor.size())\n",
    "\n",
    "print('---')\n",
    "print(tensor_2d.shape)\n",
    "print(tensor_2d.dtype)\n",
    "print(tensor_2d.device)\n",
    "print(tensor_2d.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d5c45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3., 4., 5.])\n"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, device='cpu')\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b41b1b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "torch_size = torch.Size([3, 4, 5])\n",
    "print(torch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3528e",
   "metadata": {},
   "source": [
    "## Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b090c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  4.2232,  7.0000],\n",
       "        [-2.0000,  3.0000,  6.0000]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[1.0, 4.22323, 7.0], [-2.0, 3.0, 6.0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56c623b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abs values:\n",
      "tensor([[1.0000, 4.2232, 7.0000],\n",
      "        [2.0000, 3.0000, 6.0000]]) \n",
      "\n",
      "max value:\n",
      "7.0 \n",
      "\n",
      "min value:\n",
      "-2.0 \n",
      "\n",
      "mean value:\n",
      "3.203871726989746 \n",
      "\n",
      "sum value:\n",
      "19.223230361938477 \n",
      "\n",
      "standard deviation:\n",
      "3.324061155319214 \n",
      "\n",
      "variance:\n",
      "11.049382209777832 \n",
      "\n",
      "argmax:\n",
      "2 \n",
      "\n",
      "argmin:\n",
      "3 \n",
      "\n",
      "Cos val:\n",
      "tensor([[ 0.5403, -0.4699,  0.7539],\n",
      "        [-0.4161, -0.9900,  0.9602]]) \n",
      "\n",
      "Sin val:\n",
      "tensor([[ 0.8415, -0.8827,  0.6570],\n",
      "        [-0.9093,  0.1411, -0.2794]]) \n",
      "\n",
      "Tan val:\n",
      "tensor([[ 1.5574,  1.8786,  0.8714],\n",
      "        [ 2.1850, -0.1425, -0.2910]]) \n",
      "\n",
      "log val:\n",
      "tensor([[0.0000, 1.4406, 1.9459],\n",
      "        [   nan, 1.0986, 1.7918]]) \n",
      "\n",
      "square val:\n",
      "tensor([[ 1.0000, 17.8357, 49.0000],\n",
      "        [ 4.0000,  9.0000, 36.0000]]) \n",
      "\n",
      "prod val:\n",
      "-1064.25390625 \n",
      "\n",
      "round val:\n",
      "tensor([[ 1.,  4.,  7.],\n",
      "        [-2.,  3.,  6.]]) \n",
      "\n",
      "sgn val:\n",
      "tensor([[ 1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.]]) \n",
      "\n",
      "sqrt val:\n",
      "tensor([[1.0000, 2.0550, 2.6458],\n",
      "        [   nan, 1.7321, 2.4495]]) \n",
      "\n",
      "exp val:\n",
      "tensor([[2.7183e+00, 6.8254e+01, 1.0966e+03],\n",
      "        [1.3534e-01, 2.0086e+01, 4.0343e+02]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"abs values:\\n{torch.abs(X)} \\n\")\n",
    "print(f\"max value:\\n{torch.max(X)} \\n\")\n",
    "print(f\"min value:\\n{torch.min(X)} \\n\")\n",
    "print(f\"mean value:\\n{torch.mean(X)} \\n\")\n",
    "print(f\"sum value:\\n{torch.sum(X)} \\n\")\n",
    "print(f\"standard deviation:\\n{torch.std(X)} \\n\")\n",
    "print(f\"variance:\\n{torch.var(X)} \\n\")\n",
    "print(f\"argmax:\\n{torch.argmax(X)} \\n\")\n",
    "print(f\"argmin:\\n{torch.argmin(X)} \\n\")\n",
    "print(f\"Cos val:\\n{torch.cos(X)} \\n\")\n",
    "print(f\"Sin val:\\n{torch.sin(X)} \\n\")\n",
    "print(f\"Tan val:\\n{torch.tan(X)} \\n\")\n",
    "print(f\"log val:\\n{torch.log(X)} \\n\")\n",
    "print(f\"square val:\\n{torch.square(X)} \\n\")\n",
    "print(f\"prod val:\\n{torch.prod(X)} \\n\")\n",
    "print(f\"round val:\\n{torch.round(X)} \\n\")\n",
    "print(f\"sgn val:\\n{torch.sign(X)} \\n\")\n",
    "print(f\"sqrt val:\\n{torch.sqrt(X)} \\n\")\n",
    "print(f\"exp val:\\n{torch.exp(X)} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb410da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual tensor:\n",
      "tensor([[ 1.,  4.,  7.],\n",
      "        [-2.,  3.,  6.]]) \n",
      " \n",
      "row wise max:\n",
      "torch.return_types.max(\n",
      "values=tensor([7., 6.]),\n",
      "indices=tensor([2, 2])) \n",
      "\n",
      "row wise min:\n",
      "torch.return_types.min(\n",
      "values=tensor([ 1., -2.]),\n",
      "indices=tensor([0, 0])) \n",
      "\n",
      "row wise mean:\n",
      "tensor([4.0000, 2.3333]) \n",
      "\n",
      "row wise sum:\n",
      "tensor([12.,  7.]) \n",
      "\n",
      "row wise std:\n",
      "tensor([3.0000, 4.0415]) \n",
      "\n",
      "row wise var:\n",
      "tensor([ 9.0000, 16.3333]) \n",
      "\n",
      "column wise max:\n",
      "torch.return_types.max(\n",
      "values=tensor([1., 4., 7.]),\n",
      "indices=tensor([0, 0, 0])) \n",
      "\n",
      "column wise min:\n",
      "torch.return_types.min(\n",
      "values=tensor([-2.,  3.,  6.]),\n",
      "indices=tensor([1, 1, 1])) \n",
      "\n",
      "column wise mean:\n",
      "tensor([-0.5000,  3.5000,  6.5000]) \n",
      "\n",
      "column wise sum:\n",
      "tensor([-1.,  7., 13.]) \n",
      "\n",
      "column wise std:\n",
      "tensor([2.1213, 0.7071, 0.7071]) \n",
      "\n",
      "column wise var:\n",
      "tensor([4.5000, 0.5000, 0.5000]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for row wise operations\n",
    "print(f\"actual tensor:\\n{X} \\n \")\n",
    "print(f\"row wise max:\\n{torch.max(X, dim=1)} \\n\")\n",
    "print(f\"row wise min:\\n{torch.min(X, dim=1)} \\n\")\n",
    "print(f\"row wise mean:\\n{torch.mean(X, dim=1)} \\n\")\n",
    "print(f\"row wise sum:\\n{torch.sum(X, dim=1)} \\n\")  \n",
    "print(f\"row wise std:\\n{torch.std(X, dim=1)} \\n\")\n",
    "print(f\"row wise var:\\n{torch.var(X, dim=1)} \\n\")\n",
    "# for column wise operations\n",
    "print(f\"column wise max:\\n{torch.max(X, dim=0)} \\n\")\n",
    "print(f\"column wise min:\\n{torch.min(X, dim=0)} \\n\")\n",
    "print(f\"column wise mean:\\n{torch.mean(X, dim=0)} \\n\")\n",
    "print(f\"column wise sum:\\n{torch.sum(X, dim=0)} \\n\")  \n",
    "print(f\"column wise std:\\n{torch.std(X, dim=0)} \\n\")\n",
    "print(f\"column wise var:\\n{torch.var(X, dim=0)} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1f4ce",
   "metadata": {},
   "source": [
    "## numpy <--> Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "920e16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c89271cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  4.,  7.],\n",
       "       [-2.,  3.,  6.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "738f428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c66da94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3982, 0.5601, 0.8153],\n",
      "        [0.2624, 0.8761, 0.0280],\n",
      "        [0.1548, 0.3135, 0.8533]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(3, 3)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "351ed11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3982, 0.2624, 0.1548])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 0]  # first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "041d36e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9816e-01, -9.9000e+01,  8.1527e-01],\n",
       "        [ 2.6244e-01, -9.9000e+01,  2.7971e-02],\n",
       "        [ 1.5478e-01, -9.9000e+01,  8.5327e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 1] = -99\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97f702",
   "metadata": {},
   "source": [
    "## Pytorch activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "066356c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu activation:\n",
      "tensor([[0.3982, 0.0000, 0.8153],\n",
      "        [0.2624, 0.0000, 0.0280],\n",
      "        [0.1548, 0.0000, 0.8533]])   \n",
      "\n",
      "sigmoid activation:\n",
      "tensor([[0.5982, 0.0000, 0.6932],\n",
      "        [0.5652, 0.0000, 0.5070],\n",
      "        [0.5386, 0.0000, 0.7013]])   \n",
      "\n",
      "tanh activation:\n",
      "tensor([[ 0.3784, -1.0000,  0.6725],\n",
      "        [ 0.2566, -1.0000,  0.0280],\n",
      "        [ 0.1536, -1.0000,  0.6928]])   \n",
      "\n",
      "softmax activation:\n",
      "tensor([[3.9721e-01, 2.6625e-44, 6.0279e-01],\n",
      "        [5.5835e-01, 4.3440e-44, 4.4165e-01],\n",
      "        [3.3215e-01, 2.9427e-44, 6.6785e-01]])   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'relu activation:\\n{torch.relu(X)}   \\n')\n",
    "print(f'sigmoid activation:\\n{torch.sigmoid(X)}   \\n')\n",
    "print(f'tanh activation:\\n{torch.tanh(X)}   \\n')\n",
    "print(f'softmax activation:\\n{torch.softmax(X, dim=1)}   \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6994e7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9816e-01,  2.6244e-01,  1.5478e-01],\n",
       "        [-9.9000e+01, -9.9000e+01, -9.9000e+01],\n",
       "        [ 8.1527e-01,  2.7971e-02,  8.5327e-01]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed_X = X.T\n",
    "transposed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fe0986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addition:\n",
      "tensor([[ 10.3982, -89.0000,  10.8153],\n",
      "        [ 10.2624, -89.0000,  10.0280],\n",
      "        [ 10.1548, -89.0000,  10.8533]])   \n",
      "\n",
      "subtraction:\n",
      "tensor([[  -9.6018, -109.0000,   -9.1847],\n",
      "        [  -9.7376, -109.0000,   -9.9720],\n",
      "        [  -9.8452, -109.0000,   -9.1467]])   \n",
      "\n",
      "multiplication:\n",
      "tensor([[ 3.9816e+00, -9.9000e+02,  8.1527e+00],\n",
      "        [ 2.6244e+00, -9.9000e+02,  2.7971e-01],\n",
      "        [ 1.5478e+00, -9.9000e+02,  8.5327e+00]])   \n",
      "\n",
      "division:\n",
      "tensor([[ 3.9816e-02, -9.9000e+00,  8.1527e-02],\n",
      "        [ 2.6244e-02, -9.9000e+00,  2.7971e-03],\n",
      "        [ 1.5478e-02, -9.9000e+00,  8.5327e-02]])   \n",
      "\n",
      "exponentiation:\n",
      "tensor([[1.5853e-01, 9.8010e+03, 6.6466e-01],\n",
      "        [6.8874e-02, 9.8010e+03, 7.8239e-04],\n",
      "        [2.3956e-02, 9.8010e+03, 7.2806e-01]])   \n",
      "\n",
      "matrix addition:\n",
      "tensor([[   0.7963,  -98.7376,    0.9700],\n",
      "        [ -98.7376, -198.0000,  -98.9720],\n",
      "        [   0.9700,  -98.9720,    1.7065]])   \n",
      "\n",
      "matrix element-wise multiplication:\n",
      "tensor([[ 1.5853e-01, -2.5981e+01,  1.2618e-01],\n",
      "        [-2.5981e+01,  9.8010e+03, -2.7691e+00],\n",
      "        [ 1.2618e-01, -2.7691e+00,  7.2806e-01]])   \n",
      "\n",
      "matrix multiplication:\n",
      "tensor([[9801.8232, 9801.1270, 9801.7568],\n",
      "        [9801.1270, 9801.0703, 9801.0645],\n",
      "        [9801.7568, 9801.0645, 9801.7529]])   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"addition:\\n{X + 10}   \\n\")\n",
    "print(f\"subtraction:\\n{X - 10}   \\n\")\n",
    "print(f\"multiplication:\\n{X * 10}   \\n\")\n",
    "print(f\"division:\\n{X / 10}   \\n\")\n",
    "print(f\"exponentiation:\\n{X ** 2}   \\n\")\n",
    "print(f\"matrix addition:\\n{X + transposed_X}   \\n\")\n",
    "print(f\"matrix element-wise multiplication:\\n{X * transposed_X}   \\n\")\n",
    "print(f\"matrix multiplication:\\n{X @ transposed_X}   \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d81f51",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "PyTorch comes with an efficient implementation of reverse-mode auto-differentiation called autograd, which stands for automatic gradients. It is quite easy to use. For example, consider a simple function, f(x) = x^2. Differential calculus tells us that the derivative of this function is fâ€™(x) = 2x. If we evaluate f(5) and f'(5), we get 25 and 10, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecebbd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 0., 9.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial = torch.tensor([2, 0, 3], requires_grad=True, dtype=torch.float32)  # Represents 2 + 0*x + 3*x^2\n",
    "f = polynomial ** 2  # f(x) = (2 + 0*x + 3*x^2)^2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c0081cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13., grad_fn=<SumBackward0>)\n",
      "tensor([4., 0., 6.])\n"
     ]
    }
   ],
   "source": [
    "loss = f.sum()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "print(polynomial.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f83d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(polynomial.grad)  \u001b[38;5;66;03m# df/dx = 2*(2 + 0*x + 3*x^2) * (0 + 6*x) evaluated at x=0 => 2*2*0 = 0\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\environments\\myenv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\environments\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\environments\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# df/dx = 2*(2 + 0*x + 3*x^2) * (0 + 6*x) evaluated at x=0 => 2*2*0 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d703895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "f = x ** 2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "01fa6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6f5f9a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388720fe",
   "metadata": {},
   "source": [
    "## polynomial equations and gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f04ce1a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 0., 3., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs = torch.tensor([2.0, 0.0, 3.0, 1.0], requires_grad=True)\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "412ded53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(2.0)\n",
    "f = coeffs[0] + coeffs[1] * x + coeffs[2] * x ** 2 + coeffs[3] * x ** 3\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5cfb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e76e22af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 4., 8.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5d38d27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeffs[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: f(x) = 125.0000, x = 5.0000\n",
      "x.grad: 75.0\n",
      "\n",
      "tensor(-2.5000, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 1: f(x) = -15.6250, x = -2.5000\n",
      "x.grad: 18.75\n",
      "\n",
      "tensor(-4.3750, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 2: f(x) = -83.7402, x = -4.3750\n",
      "x.grad: 57.421875\n",
      "\n",
      "tensor(-10.1172, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 3: f(x) = -1035.5698, x = -10.1172\n",
      "x.grad: 307.07244873046875\n",
      "\n",
      "tensor(-40.8244, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 4: f(x) = -68039.3984, x = -40.8244\n",
      "x.grad: 4999.90283203125\n",
      "\n",
      "tensor(-540.8147, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 5: f(x) = -158177776.0000, x = -540.8147\n",
      "x.grad: 877441.625\n",
      "\n",
      "tensor(-88284.9766, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 6: f(x) = -688114024579072.0000, x = -88284.9766\n",
      "x.grad: 23382710272.0\n",
      "\n",
      "tensor(-2.3384e+09, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 7: f(x) = -12785971354604844136253095936.0000, x = -2338359296.0000\n",
      "x.grad: 1.6403772304464544e+19\n",
      "\n",
      "tensor(-1.6404e+18, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 8: f(x) = -inf, x = -1640377202958663680.0000\n",
      "x.grad: 8.072512476982085e+36\n",
      "\n",
      "tensor(-8.0725e+35, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 9: f(x) = -inf, x = -807251231852576022673376477968334848.0000\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 10: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 11: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 12: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 13: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 14: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 15: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 16: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 17: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 18: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 19: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 20: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 21: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 22: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 23: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 24: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 25: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 26: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 27: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 28: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 29: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 30: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 31: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 32: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 33: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 34: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 35: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 36: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 37: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 38: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 39: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 40: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 41: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 42: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 43: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 44: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 45: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 46: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 47: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 48: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 49: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 50: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 51: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 52: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 53: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 54: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 55: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 56: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 57: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 58: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 59: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 60: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 61: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 62: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 63: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 64: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 65: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 66: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 67: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 68: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 69: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 70: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 71: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 72: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 73: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 74: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 75: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 76: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 77: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 78: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 79: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 80: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 81: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 82: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 83: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 84: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 85: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 86: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 87: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 88: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 89: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 90: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 91: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 92: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 93: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 94: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 95: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 96: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 97: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 98: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n",
      "Iteration 99: f(x) = -inf, x = -inf\n",
      "x.grad: inf\n",
      "\n",
      "tensor(-inf, requires_grad=True)\n",
      "x.grad zeroed: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "for iteration in range(100):\n",
    "    f = x ** 3  # forward pass\n",
    "    print(f\"Iteration {iteration}: f(x) = {f.item():.4f}, x = {x.item():.4f}\")\n",
    "    f.backward()  # backward pass\n",
    "    print(f\"x.grad: {x.grad}\\n\") # 3*x**2\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # gradient descent step\n",
    "        print(x) # reset the gradients\n",
    "\n",
    "    x.grad.zero_()  # reset the gradients\n",
    "    print(f\"x.grad zeroed: {x.grad}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93b7fdd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0185e-09, requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70eceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
